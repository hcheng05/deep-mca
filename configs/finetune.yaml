model:
  hidden_size: 256
  num_layers: 24
  state_size: 16
  dropout: 0.05
  pretrained_path: null

data:
  dataset: Arcticbun/skl_x86
  vocab_path: data/vocab.pkl
  max_seq_len: 512
  train_ratio: 0.8

training:
  batch_size: 64
  lr: 5e-4
  weight_decay: 0.01
  epochs: 60
  warmup_ratio: 0.03
  log_interval: 50
  checkpoint_dir: checkpoints

wandb:
  project: deep-mca-finetune
  entity: mamba-mca
  name: null
